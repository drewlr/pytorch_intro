{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREDITS\n",
    "\n",
    "Original inspiration from: https://github.com/yang-zhang/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example: y = scalar, x = scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x =  tensor(2., requires_grad=True)\n",
      "     y =  tensor(12., grad_fn=<MulBackward0>)\n",
      "x.grad =  None\n",
      "\n",
      "After calling y.backward() once: we calculate the grad of y w.r.t. x, and assign this value to x.grad\n",
      "\n",
      "     x =  tensor(2., requires_grad=True)\n",
      "     y =  tensor(12., grad_fn=<MulBackward0>)\n",
      "x.grad =  tensor(12.)\n",
      "   6*x =  tensor(12., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(2.0, requires_grad=True)\n",
    "y = 3*x**2\n",
    "print('     x = ', x)\n",
    "print('     y = ', y)\n",
    "print('x.grad = ', x.grad)\n",
    "print('\\nAfter calling y.backward() once: we calculate the grad of y w.r.t. x, and assign this value to x.grad\\n')\n",
    "y.backward()\n",
    "print('     x = ', x)\n",
    "print('     y = ', y)\n",
    "print('x.grad = ', x.grad)\n",
    "print('   6*x = ', 6*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulating gradients** \n",
    "\n",
    "If now we create another variable as a function of x, and calculate the grad...we will notice that the grad does not match the derivative. Reason for this, is that x.grad accumulates the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad =  tensor(12.)\n",
      "\n",
      "Calling z.backward():\n",
      "     x =  tensor(2., requires_grad=True)\n",
      "     z =  tensor(16., grad_fn=<MulBackward0>)\n",
      "   8*x =  tensor(16., grad_fn=<MulBackward0>)\n",
      "x.grad =  tensor(28.)\n",
      "8*x + previous_grad =  tensor(28., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#x = tensor(2.0, requires_grad=True)\n",
    "print('x.grad = ', x.grad)\n",
    "previous_grad = x.grad.clone()\n",
    "z = 4*x**2\n",
    "\n",
    "print('\\nCalling z.backward():')\n",
    "z.backward()\n",
    "print('     x = ', x)\n",
    "print('     z = ', z)\n",
    "\n",
    "print('   8*x = ', 8*x)\n",
    "print('x.grad = ', x.grad)\n",
    "print('8*x + previous_grad = ', 8*x + previous_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can address this by setting x.grad = None, or by calling zero_grad() while training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad =  None\n"
     ]
    }
   ],
   "source": [
    "x.grad = None\n",
    "print('x.grad = ', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y = vector, x = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output grad = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "  tensor([[0.2873],\n",
      "        [0.3655]], requires_grad=True)\n",
      "\n",
      "y= x[0]**3 + 2*x[1]**2:\n",
      "  tensor([0.2910], grad_fn=<AddBackward0>)\n",
      "\n",
      "x.grad:\n",
      "  None\n",
      "\n",
      "---\n",
      "After calling y.backward() once: we calculate the grad of y w.r.t. x, and assign this value to x.grad\n",
      "\n",
      "x:\n",
      "  tensor([[0.2873],\n",
      "        [0.3655]], requires_grad=True)\n",
      "y:\n",
      "  tensor([0.2910], grad_fn=<AddBackward0>)\n",
      "\n",
      "x.grad:\n",
      "  tensor([[0.2477],\n",
      "        [1.4622]])\n",
      "\n",
      "3*x[0]**2, 4*x[1]:\n",
      "  tensor([0.2477], grad_fn=<MulBackward0>) \n",
      " tensor([1.4622], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 1).requires_grad_(True)\n",
    "y = x[0]**3 + 2*x[1]**2\n",
    "\n",
    "print('x:\\n ', x)\n",
    "print('\\ny= x[0]**3 + 2*x[1]**2:\\n ', y)\n",
    "print('\\nx.grad:\\n ', x.grad)\n",
    "\n",
    "print('\\n---\\nAfter calling y.backward() once: we calculate the grad of y w.r.t. x, and assign this value to x.grad\\n')\n",
    "y.backward(gradient=torch.ones(1))\n",
    "print('x:\\n ', x)\n",
    "print('y:\\n ', y)\n",
    "print('\\nx.grad:\\n ', x.grad)\n",
    "print('\\n3*x[0]**2, 4*x[1]:\\n ', 3*x[0]**2,'\\n', 4*x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x = scalar, y = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "  tensor([0.4187], requires_grad=True)\n",
      "\n",
      " y[0] = x**3\n",
      "\n",
      " y[1] = 2*x**2\n",
      "\n",
      "x.grad:\n",
      "  None\n",
      "\n",
      "---\n",
      "After calling y.backward() once: we calculate the grad of y w.r.t. x, and assign this value to x.grad\n",
      "\n",
      "x:\n",
      "  tensor([0.4187], requires_grad=True)\n",
      "y:\n",
      "  tensor([[0.0734],\n",
      "        [0.3505]], grad_fn=<CopySlices>)\n",
      "\n",
      "x.grad:\n",
      "  tensor([2.2004])\n",
      "\n",
      "3*x**2 + 4*x:\n",
      "  tensor([2.2004], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.rand(1), requires_grad=True)\n",
    "y = Variable(torch.zeros(2, 1), requires_grad=False)\n",
    "y[0] = x**3\n",
    "y[1] = 2*x**2\n",
    "\n",
    "print('x:\\n ', x)\n",
    "print('\\n y[0] = x**3')\n",
    "print('\\n y[1] = 2*x**2')\n",
    "print('\\nx.grad:\\n ', x.grad)\n",
    "\n",
    "print('\\n---\\nAfter calling y.backward() once: we calculate the grad of y w.r.t. x, and assign this value to x.grad\\n')\n",
    "y.backward(gradient=torch.ones(y.size()))\n",
    "print('x:\\n ', x)\n",
    "print('y:\\n ', y)\n",
    "print('\\nx.grad:\\n ', x.grad)\n",
    "print('\\n3*x**2 + 4*x:\\n ', 3*x**2 + 4*x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
